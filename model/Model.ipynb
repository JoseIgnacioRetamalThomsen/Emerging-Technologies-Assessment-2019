{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig a Model Using the MNIST datase for recgonise hand-writen digits\n",
    "    On this notebook we show how to create a model using  MNIST dataset.\n",
    "    We start showing how to read the bits from the dataset, then we do a quick introduction to neural network what \n",
    "    is useful for understand the model thath we will train.\n",
    "    We start with a simple linear model, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "  We will start by undertandig how the data is formated and parsing it i a suuitable way for train our model.\n",
    "  \n",
    "  Mnist provide 4 files: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little and Big Endian Architecture \n",
    "\n",
    "  There are 2 types of processors architecture(litle and big endian). In litle endian bits are store from left to righ This is basically how the bytes are stored, in litle they are stored from left to right and in big the other way around.(Look this is you want to know more about https://chortle.ccsu.edu/AssemblyTutorial/Chapter-15/ass15_3.html ).\n",
    "  This is relevant for us because we need to know read the bytes right for get the proper data.\n",
    "  In python we can easyle check using sys, since I am using a Intel processor i expected to be litle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sys import byteorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check  our architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little\n"
     ]
    }
   ],
   "source": [
    "print(byteorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Mnist\n",
    "\n",
    "  Now we can start reading the train images file. \n",
    "  From Mnist webstie we know what to expected from each bit readed.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open('mnist/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    fc_train_img = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The first 4 bytes is the magic number which is a 32 bit integer, for the image set this number is 2051. Note that we are setting the byteorder as big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2051"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(fc_train_img[0:4], byteorder='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next is the the number of images as a 32 bit integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_total = int.from_bytes(fc_train_img[4:8], byteorder='big')\n",
    "train_img_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then number of rows as 32 bit integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = int.from_bytes(fc_train_img[8:12], byteorder='big')\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a columns as 32 bit integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = int.from_bytes(fc_train_img[12:16], byteorder='big')\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then unsigned bytes(8 bits) , each byte represent a pixel. They are orginezed row-wise.\n",
    "\n",
    "The total of bits is : \n",
    "```python\n",
    "train_img_total*row*col\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47040016"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_byte = (train_img_total*row*col) + 16\n",
    "last_byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whe can read all bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(fc_train_img[16:last_byte])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reshape them ass 28*27 (784) array. They represent our vector.\n",
    "\n",
    "Also in Mnist pixel values are 0 to 255.0 , 0 representing the background. We want ot invert this, because having a non zero value as background is better for the trainig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = ~np.array(x_train).reshape(train_img_total,row*col).astype(np.uint8)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see one image using pyplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ddd22d64c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[11].reshape(row,col), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats look like the image 11 is a 5. We can now read labels and check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('mnist/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    fc_train_lbl = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 32 bits are magin number : 2049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2049"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(fc_train_lbl[0:4], byteorder='big')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a 32 bits intiger, the total of labels. Must be 6000 as we got 6000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lvl_total = int.from_bytes(fc_train_lbl[4:8], byteorder='big')\n",
    "train_lvl_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each unsigned byte is a label, so we can check that 11 is 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(list(fc_train_lbl[8:train_lvl_total+8]))\n",
    "print(y_train[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now real the test images and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('mnist/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    fc_test_img = f.read()\n",
    "with gzip.open('mnist/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    fc_test_lbl = f.read()\n",
    "    \n",
    "test_img_total = int.from_bytes(fc_test_img[4:8], byteorder='big')\n",
    "test_lbl_total = int.from_bytes(fc_test_lbl[4:8], byteorder='big')\n",
    "\n",
    "last_byte = (test_img_total*row*col) + 16\n",
    "x_test = list(fc_test_img[16:last_byte])\n",
    "x_test = ~np.array(x_test).reshape(test_img_total,row*col).astype(np.uint8)\n",
    "\n",
    "y_test = np.array(list(fc_test_lbl[8:test_lbl_total+8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =  x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ddcffbf248>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[11].reshape(row,col), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(y_test[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Network\n",
    "\n",
    "Most peope can recognize handwrite digits with no problem(beacuse we have a very complex brain). This is not a easy task for a computer. We will use the neural netwok aproach, we will take a large number of labeled handwrite digits -the training examples- (<b>x_train</b>), and create a model that can learn from those examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons\n",
    "\n",
    " The idea is simple, they take several imputs(  $ x_1, x_2 ... x_n $) and produce a single output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single neuron](img/SingleNeuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input have  <i>weights</i>  $ w_1, w_2 ... w_n $ , a weight sum is calculated:${\\sum_n x_n w_n}$ then the results goes through a activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[http://neuralnetworksanddeeplearning.com/chap1.html](http://neuralnetworksanddeeplearning.com/chap1.html/) \n",
    "\n",
    "[https://github.com/ianmcloughlin/jupyter-teaching-notebooks/blob/master/keras-neurons.ipynb](https://github.com/ianmcloughlin/jupyter-teaching-notebooks/blob/master/keras-neurons.ipynb) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "We create layers composed of neurons. at least one input layer and a output layer is required, there can be hidden layers between the input and output layer. \n",
    "\n",
    "Our images are 28x28, we can create a 784 vector that represent the image. So we will feed out network with 784 inputs, this is our input layer that is also know as hidden layer.\n",
    "Since we need to discriminate 10 numbers out output layer will have 10 neurons, each will represent a digit from 0 to 9. Then we can use SoftMax activation function for the output of each of our network to represent a probability of being a digit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation : softmax\n",
    "  We want our result to represent a probablility of being one of the 10 digits(0,1,3...).\n",
    "  The softmax activation function will do this for us, it will output our output for each neuron beetwen 0 and 1.Then each \n",
    "  of our neuron can represent a a digit so the one will bigger probability will be our prediction.\n",
    "  \n",
    "  $$\\sigma (z)_j = \\frac{e^{(z)_j}}{\\sum_{k=0}^{K}e^{(z)_k}} \\text{  where   j = 1,...,K}$$  \n",
    "  \n",
    "  In our case K = 10.\n",
    "  \n",
    "  https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model\n",
    "\n",
    " We will start create a 2 layers model, input and output : the input layer will be the 784 vector created with the image data and the output a 10 neuros with activation softmax. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single neuron](img/Model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequeantial model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# The input layer is added by keras when we set input = 784 \n",
    "# We add 10 neuroas with softmax for output\n",
    "model.add(kr.layers.Dense(units=10, input_dim=784, activation='softmax'))\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can se a summary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7850 parameters are:\n",
    " 10 weights from each input node to each output made 7840 plus 10 weightns on the outputs nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are missing few thinks to explain: loss,optimizer and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.argmax(result, axis=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss : categorical_crossentropy\n",
    "\n",
    "    After we calculate our result we need to calculate the distance beetween our prediction and our epected result.\n",
    "    https://algorithmia.com/blog/introduction-to-loss-functions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimazer : adam\n",
    "https://algorithmia.com/blog/introduction-to-optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the labes\n",
    "    \n",
    "We are almost done for train our model for first time, the only problems is that our labels are a single digit. We need make theem vectors with a 1 on the number position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = kr.utils.to_categorical(y_train, 10)\n",
    "y_test  = kr.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready, we will start training this simple model with 10 epochs, batch of 100 (means we train the model with 100 images at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.8040 - accuracy: 0.7862 - val_loss: 0.4491 - val_accuracy: 0.8792\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.4267 - accuracy: 0.8811 - val_loss: 0.3719 - val_accuracy: 0.8945\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3743 - accuracy: 0.8922 - val_loss: 0.3385 - val_accuracy: 0.9017\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3517 - accuracy: 0.8982 - val_loss: 0.3283 - val_accuracy: 0.9082\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3380 - accuracy: 0.9019 - val_loss: 0.3115 - val_accuracy: 0.9094\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3278 - accuracy: 0.9056 - val_loss: 0.3049 - val_accuracy: 0.9104\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3227 - accuracy: 0.9058 - val_loss: 0.3076 - val_accuracy: 0.9110\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3161 - accuracy: 0.9089 - val_loss: 0.3101 - val_accuracy: 0.9117\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3118 - accuracy: 0.9105 - val_loss: 0.2953 - val_accuracy: 0.9172\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3090 - accuracy: 0.9115 - val_loss: 0.2884 - val_accuracy: 0.9174\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First results\n",
    "\n",
    " We have achived a 99.67 accuracy with this very simple model, this is going to be our baseline and we change our model for improve the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get data history, since we will be doing this several times i also define a function for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = np.array(history_callback.history['val_accuracy'])\n",
    "val_loss =  np.array(history_callback.history['val_loss'])\n",
    "accuracy =  np.array(history_callback.history['accuracy'])\n",
    "loss =  np.array(history_callback.history['loss'])\n",
    "\n",
    "def get_stats():\n",
    "    val_accuracy = np.array(history_callback.history['val_accuracy'])\n",
    "    val_loss =  np.array(history_callback.history['val_loss'])\n",
    "    accuracy =  np.array(history_callback.history['accuracy'])\n",
    "    loss =  np.array(history_callback.history['loss'])\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1df65a36f48>,\n",
       " <matplotlib.lines.Line2D at 0x1df65a3de48>,\n",
       " <matplotlib.lines.Line2D at 0x1df65a3df88>,\n",
       " <matplotlib.lines.Line2D at 0x1df65a47208>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, epoch, 1)\n",
    "plt.plot(x,val_accuracy,'b',x,accuracy,'r',x,loss,'g',x,val_loss,'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Red : Test accuracy.\n",
    "- Blue : Train accuracy.\n",
    "- Green : Train loss.\n",
    "- Yellow: Test loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding more Layers and Relu activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu\n",
    "\n",
    "Rectified linea unit(Relu), is one of the most common activation functions in neural networks. Relu is very simple, linear for all positive valueas and 0 for all negative values:\n",
    "$$ f(x)       \\begin{cases}\n",
    "       \\text{0,} &\\quad\\text{if } x \\le0\\\\\n",
    "       f(x) &\\quad\\text{if } x  >0 \\\\\n",
    "       \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Relu](img/relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some advantages of Relu:\n",
    "- Cheap to compute.\n",
    "- Good sparsity.\n",
    "- Converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://keras.io/activations/](https://keras.io/activations/)\n",
    "\n",
    "[https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7) \n",
    "\n",
    "[https://datascience.stackexchange.com/questions/14349/difference-of-activation-functions-in-neural-networks-in-general](https://datascience.stackexchange.com/questions/14349/difference-of-activation-functions-in-neural-networks-in-general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra layers\n",
    "\n",
    "We can now add more layers using relu activation function:\n",
    "- Same input layer.\n",
    "- \" hiddens laters  512 and 98 neuors, with relu.\n",
    "- Same output layer, 10 neurons with softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(units=512, input_dim=784, activation='relu'))\n",
    "model.add(kr.layers.Dense(units=98, activation='relu'))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 98)                50274     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                990       \n",
      "=================================================================\n",
      "Total params: 453,184\n",
      "Trainable params: 453,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "28500/60000 [=============>................] - ETA: 0s - loss: 0.6904 - accuracy: 0.7917"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getStats()\n",
    "x = np.arange(0.0, epoch, 1)\n",
    "plt.plot(x,val_accuracy,'b',x,accuracy,'r',x,loss,'g',x,val_loss,'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu and the extra layers have give a greeat improvement to our model, from 91% to 96.5%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Drop out is regularization technic, basically we drop some random neurons during trainig, \n",
    "they are temporary shuttend down. \n",
    "\n",
    "Is belive that this make other neurons be more active so improve the learning rate.\n",
    "\n",
    "The effect is a network taht is less sensitive to specific weights of neurons. The netwoek is capable of a better generalizaion and avoid overfit.\n",
    "\n",
    "Drop out is very easily implemented with keras, we just need to pick a percent of dropped out neurons for each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://keras.io/layers/core/](https://keras.io/layers/core/)\n",
    "\n",
    "[https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(units=512, input_dim=784, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.01))\n",
    "model.add(kr.layers.Dense(units=98, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getStats()\n",
    "x = np.arange(0.0, epoch, 1)\n",
    "plt.plot(x,val_accuracy,'b',x,accuracy,'r',x,loss,'g',x,val_loss,'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout improve our model from 96.5% to 97%, so now we are at 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Convolutional Neural Network\n",
    "\n",
    "We have achiave 97% accuracy on our model, but we are missing a very importan point : shapes are 2D images and there is a relation beetween pixels. Transforming our image in a 1D vector we completaly ignore the shape of the image.\n",
    "Using convolutional neurons we can feed neurons with 2D arrays of pixels and someone preserve the data on the shape of the image.\n",
    "\n",
    "In the image bellow we can see how a 3x3 convolutional networds works.Note that the image is just for ilustrate how convolutional networls works, is not related with the one we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Conv2d](img/conv2d.gif)\n",
    "\n",
    "### image from:  [https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/](https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://keras.io/layers/convolutional/](https://keras.io/layers/convolutional/)\n",
    "\n",
    "[https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reforating input data for convolutional network\n",
    "\n",
    "We now need to reshape our data for be a 28 by 28 array so it can be fit onto the convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(train_img_total,col,row,1)\n",
    "x_test = x_test.reshape(test_img_total,col,row,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "\n",
    "model.add(kr.layers.Conv2D(256,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dropout(0.25))\n",
    "model.add(kr.layers.Dense(units=98, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getStats()\n",
    "x = np.arange(0.0, epoch, 1)\n",
    "plt.plot(x,val_accuracy,'b',x,accuracy,'r',x,loss,'g',x,val_loss,'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 1 convolutional layer we have improve our accuracy to 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More layers and max polling\n",
    "\n",
    "A polling layer is a layer that we add aftter a convolutional layer.  The polling layer will operate over each pixel for create a new set of pulled data. We will use max polling of size 2x2 pixels. This means that the maximun of each 2x2 patch will be calculated.\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- Model invariance to local translation.\n",
    "- Faster computations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MAxPolling2d](img/maxpolling2d.png)\n",
    "\n",
    "image from: [https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/](https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://keras.io/layers/pooling/](https://keras.io/layers/pooling/)\n",
    "\n",
    "[https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Conv2D(128,kernel_size=(5, 5),activation='relu'))\n",
    "model.add(kr.layers.Conv2D(256,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dropout(0.25))\n",
    "model.add(kr.layers.Dense(units=98, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Conv2D(128,kernel_size=(5, 5),activation='relu'))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(256,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dropout(0.25))\n",
    "model.add(kr.layers.Dense(units=98, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(kr.callbacks.Callback):\n",
    "    minimun = 0.99\n",
    "   \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        score = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        # logs is a dictionary\n",
    "        print(f\"epoch: {epoch},history_callback.{logs['val_accuracy']}\")\n",
    "        print(score[1])\n",
    "        if score[1] > self.minimun: # your custom condition\n",
    "    \n",
    "            self.model.save('model7.h5', overwrite=True)\n",
    "           \n",
    "            \n",
    "            \n",
    "           \n",
    "            \n",
    "            self.minimun = score[1]\n",
    "            print(self.minimun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbk = CustomModelCheckpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "kr.layers.normalization.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "kr.layers.normalization.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Conv2D(128,kernel_size=(5, 5),activation='relu'))\n",
    "model.add(kr.layers.Conv2D(256,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Dropout(0.25))\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss=kr.losses.categorical_crossentropy,optimizer='adadelta',metrics=['accuracy'])\n",
    "model.summary()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.flow(x_train, y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch =50\n",
    "history_callback= model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=epoch, \n",
    "                    validation_data=test_generator, validation_steps=10000//64, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = kr.models.load_model('./model7.h5').evaluate(x_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
