{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig a Model Using the MNIST datase for recgonise hand-writen digits\n",
    "    On this notebook we show how to create a model using  MNIST dataset.\n",
    "    We start showing how to read the bits from the dataset, then we do a quick introduction to neural network what \n",
    "    is useful for understand the model thath we will train.\n",
    "    We start with a simple linear model, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "  We will start by undertandig how the data is formated and parsing it i a suuitable way for train our model.\n",
    "  \n",
    "  Mnist provide 4 files: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little and Big Endian Architecture \n",
    "\n",
    "  There are 2 types of processors architecture(litle and big endian). In litle endian bits are store from left to righ This is basically how the bytes are stored, in litle they are stored from left to right and in big the other way around.(Look this is you want to know more about https://chortle.ccsu.edu/AssemblyTutorial/Chapter-15/ass15_3.html ).\n",
    "  This is relevant for us because we need to know read the bytes right for get the proper data.\n",
    "  In python we can easyle check using sys, since I am using a Intel processor i expected to be litle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sys import byteorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check  our architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little\n"
     ]
    }
   ],
   "source": [
    "print(byteorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Mnist\n",
    "\n",
    "  Now we can start reading the train images file. \n",
    "  From Mnist webstie we know what to expected from each bit readed.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open('mnist/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    fc_train_img = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The first 4 bytes is the magic number which is a 32 bit integer, for the image set this number is 2051. Note that we are setting the byteorder as big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2051"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(fc_train_img[0:4], byteorder='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next is the the number of images as a 32 bit integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_total = int.from_bytes(fc_train_img[4:8], byteorder='big')\n",
    "train_img_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then number of rows as 32 bit integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = int.from_bytes(fc_train_img[8:12], byteorder='big')\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a columns as 32 bit integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = int.from_bytes(fc_train_img[12:16], byteorder='big')\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then unsigned bytes(8 bits) , each byte represent a pixel. They are orginezed row-wise.\n",
    "\n",
    "The total of bits is : \n",
    "```python\n",
    "train_img_total*row*col\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47040016"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_byte = (train_img_total*row*col) + 16\n",
    "last_byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whe can read all bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(fc_train_img[16:last_byte])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reshape them ass 28*27 (784) array. They represent our vector.\n",
    "\n",
    "Also in Mnist pixel values are 0 to 255.0 , 0 representing the background. We want ot invert this, because having a non zero value as background is better for the trainig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = ~np.array(x_train).reshape(train_img_total,row*col).astype(np.uint8)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see one image using pyplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24383ce8348>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[11].reshape(row,col), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats look like the image 11 is a 5. We can now read labels and check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('mnist/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    fc_train_lbl = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 32 bits are magin number : 2049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2049"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(fc_train_lbl[0:4], byteorder='big')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a 32 bits intiger, the total of labels. Must be 6000 as we got 6000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lvl_total = int.from_bytes(fc_train_lbl[4:8], byteorder='big')\n",
    "train_lvl_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each unsigned byte is a label, so we can check that 11 is 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(list(fc_train_lbl[8:train_lvl_total+8]))\n",
    "print(y_train[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now real the test images and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('mnist/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    fc_test_img = f.read()\n",
    "with gzip.open('mnist/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    fc_test_lbl = f.read()\n",
    "    \n",
    "test_img_total = int.from_bytes(fc_test_img[4:8], byteorder='big')\n",
    "test_lbl_total = int.from_bytes(fc_test_lbl[4:8], byteorder='big')\n",
    "\n",
    "last_byte = (test_img_total*row*col) + 16\n",
    "x_test = list(fc_test_img[16:last_byte])\n",
    "x_test = ~np.array(x_test).reshape(test_img_total,row*col).astype(np.uint8)\n",
    "\n",
    "y_test = np.array(list(fc_test_lbl[8:test_lbl_total+8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =  x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2438465a488>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAObklEQVR4nO3df6xU9ZnH8c8jxUQtJihXww9XuhWixkTACcHcTaPUJf6IICY1xYSwBrzGH0mL/LGmK9YY4o/NlkbNhngRUnbTtTRpVaJEa7CJKVHCCKziklXWsJRyhUvUINHYRZ794x7MFe98zzDnzJyB5/1KJjNznjlzngx87pmZ7znzNXcXgNPfGVU3AKAzCDsQBGEHgiDsQBCEHQjiO53c2Lhx43zy5Mmd3CQQyp49e3To0CEbqVYo7GZ2vaQnJY2S9Ky7P556/OTJk1Wv14tsEkBCrVZrWGv5bbyZjZL0r5JukHS5pAVmdnmrzwegvYp8Zp8pabe7f+juf5X0G0nzymkLQNmKhH2ipD8Pu78vW/YNZtZnZnUzqw8ODhbYHIAiioR9pC8BvnXsrbv3u3vN3Ws9PT0FNgegiCJh3yfpomH3J0naX6wdAO1SJOxbJU0xs++Z2ZmSfixpQzltAShby0Nv7n7UzO6T9KqGht7Wuvt7pXUGoFSFxtndfaOkjSX1AqCNOFwWCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgOvpT0mjNl19+maz39vY2rG3fvj257s0335ysv/DCC8k6Th3s2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZu0DeOPrSpUuT9R07djSsmY04e+/XrrrqqmQdpw/27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsXeCpp55K1vv7+5P12bNnN6w98sgjyXVnzZqVrOP0USjsZrZH0meSvpJ01N1rZTQFoHxl7NmvdfdDJTwPgDbiMzsQRNGwu6Q/mNnbZtY30gPMrM/M6mZWHxwcLLg5AK0qGvZed58h6QZJ95rZD058gLv3u3vN3Ws9PT0FNwegVYXC7u77s+uDkp6XNLOMpgCUr+Wwm9k5Zjbm+G1JcyTtLKsxAOUq8m38hZKez86X/o6k/3D3V0rpKpiBgYFC61933XUNa4yj47iWw+7uH0q6ssReALQRQ29AEIQdCIKwA0EQdiAIwg4EwSmuXeDIkSPJ+ujRo5P11NAbcBx7diAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2Dti/f3+yvmbNmmT96quvTtZnzJhx0j0hHvbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wdsGLFiqpbOCW9+eabyfq+fftafu4rr0z/MPLUqVNbfu5uxZ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0DXn755ULrL1mypKROOu/uu+9uWMt7XT755JNk/YsvvmipJ0k699xzk/WlS5cm68uXL29521XJ3bOb2VozO2hmO4ctO8/MXjOzD7Lrse1tE0BRzbyN/5Wk609Y9oCkTe4+RdKm7D6ALpYbdnd/Q9LHJyyeJ2lddnudpFtK7gtAyVr9gu5Cdx+QpOz6gkYPNLM+M6ubWX1wcLDFzQEoqu3fxrt7v7vX3L3W09PT7s0BaKDVsB8ws/GSlF0fLK8lAO3Qatg3SFqU3V4k6cVy2gHQLubu6QeYPSfpGknjJB2Q9HNJL0j6raS/kbRX0o/c/cQv8b6lVqt5vV4v2HL3+fzzz5P1KVOmJOujRo1K1vfu3XvSPTXr6NGjyfq2bduS9fnz5yfrH330UcPasWPHkuvmfezr7e1N1lO9572mEydOTNY3b96crF988cXJervUajXV63UbqZZ7UI27L2hQ+mGhrgB0FIfLAkEQdiAIwg4EQdiBIAg7EASnuJbg2WefTdYPHDiQrPf19ZXZzjfkTRfd39+frBf9GewJEyY0rC1cuDC57j333JOsT5o0qaWeJGnu3LnJ+saNG5P1gYGBZL2qobcU9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7CXYvn17ofXzToEtIm+c/JlnnknWzUY8W/Jrs2fPTtZXrlzZsHbFFVck122nSy65pLJtV4U9OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTh7CfLOGW+3999/v2Ft/fr1hZ77zjvvTNaffPLJZP3MM88stP2qTJ8+PVmfMWNGhzopD3t2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfYSHD58OFnPmxY7r57n6aefblj79NNPk+vefvvtyfqqVata6qnbHTlyJFnPOz7gVDx+IHfPbmZrzeygme0ctuxhM/uLme3ILje2t00ARTXzNv5Xkq4fYfkv3X1adklPnwGgcrlhd/c3JH3cgV4AtFGRL+juM7N3srf5Yxs9yMz6zKxuZvXBwcECmwNQRKthXyXp+5KmSRqQ9ItGD3T3fnevuXutp6enxc0BKKqlsLv7AXf/yt2PSVotaWa5bQEoW0thN7Pxw+7Ol7Sz0WMBdIfccXYze07SNZLGmdk+ST+XdI2ZTZPkkvZIuquNPXa9M85I/83M++31vHqe1Fzhec+dN8/4qSz1OwNr1qxJrnvrrbeW3U7lcsPu7gtGWJx+pQB0HQ6XBYIg7EAQhB0IgrADQRB2IAhOcT0NpKZd3rx5c3LdvPqjjz6arN91V3rU9fzzz0/W2yk1fHbWWWcl1122bFnZ7VSOPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4e5NSp0tWPWVzaix727ZtyXXnzp2brD/00EPJ+quvvpqsv/TSSw1rY8aMaXldSVqxYkWyvn379oa1Bx98MLnurFmzkvVTEXt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfYmTZgwoWFt6tSpyXX37t2brL/++uvJet4542effXbD2vjx4xvWJGnr1q3Jet5Y92WXXZasp6aMzjtnPO/nnvPOSU+NpS9fvjy57umIPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4ewnyxoNvuummZH3jxo3J+pw5c5L1+++/v2Etb5w9z5YtW5L1xx57rOX13T25bt7xC3m/aT9//vxkPZrcPbuZXWRmfzSzXWb2npn9JFt+npm9ZmYfZNdj298ugFY18zb+qKRl7n6ZpFmS7jWzyyU9IGmTu0+RtCm7D6BL5Ybd3QfcfVt2+zNJuyRNlDRP0rrsYesk3dKuJgEUd1Jf0JnZZEnTJW2RdKG7D0hDfxAkXdBgnT4zq5tZfXBwsFi3AFrWdNjN7LuSfifpp+5+uNn13L3f3WvuXuvp6WmlRwAlaCrsZjZaQ0H/tbv/Plt8wMzGZ/Xxkg62p0UAZcgdejMzk7RG0i53XzmstEHSIkmPZ9cvtqXDU8CkSZOS9VdeeSVZv/baa5P1t956K1m/7bbbkvWUvOGvoX/+9rjjjjuS9SeeeCJZr3I66FNRM+PsvZIWSnrXzHZky36moZD/1swWS9or6UftaRFAGXLD7u5/ktToz/sPy20HQLtwuCwQBGEHgiDsQBCEHQiCsANBcIprB+SdZpo3jr5+/fpkfffu3Q1rq1evTq67ZMmSZL3oOPvixYsb1i699NJCz42Tw54dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4KwvPOZy1Sr1bxer3dse0A0tVpN9Xp9xIMj2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAELlhN7OLzOyPZrbLzN4zs59kyx82s7+Y2Y7scmP72wXQqmYmiTgqaZm7bzOzMZLeNrPXstov3f1f2tcegLI0Mz/7gKSB7PZnZrZL0sR2NwagXCf1md3MJkuaLmlLtug+M3vHzNaa2dgG6/SZWd3M6oODg4WaBdC6psNuZt+V9DtJP3X3w5JWSfq+pGka2vP/YqT13L3f3WvuXuvp6SmhZQCtaCrsZjZaQ0H/tbv/XpLc/YC7f+XuxyStljSzfW0CKKqZb+NN0hpJu9x95bDlw6cmnS9pZ/ntAShLM9/G90paKOldM9uRLfuZpAVmNk2SS9oj6a62dAigFM18G/8nSSP9DvXG8tsB0C4cQQcEQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQjC3L1zGzMblPS/wxaNk3SoYw2cnG7trVv7kuitVWX2drG7j/j7bx0N+7c2blZ391plDSR0a2/d2pdEb63qVG+8jQeCIOxAEFWHvb/i7ad0a2/d2pdEb63qSG+VfmYH0DlV79kBdAhhB4KoJOxmdr2Z/beZ7TazB6rooREz22Nm72bTUNcr7mWtmR00s53Dlp1nZq+Z2QfZ9Yhz7FXUW1dM452YZrzS167q6c87/pndzEZJel/S30vaJ2mrpAXu/l8dbaQBM9sjqebulR+AYWY/kHRE0r+5+xXZsn+W9LG7P579oRzr7v/YJb09LOlI1dN4Z7MVjR8+zbikWyT9gyp87RJ93aYOvG5V7NlnStrt7h+6+18l/UbSvAr66Hru/oakj09YPE/Suuz2Og39Z+m4Br11BXcfcPdt2e3PJB2fZrzS1y7RV0dUEfaJkv487P4+ddd87y7pD2b2tpn1Vd3MCC509wFp6D+PpAsq7udEudN4d9IJ04x3zWvXyvTnRVUR9pGmkuqm8b9ed58h6QZJ92ZvV9Gcpqbx7pQRphnvCq1Of15UFWHfJ+miYfcnSdpfQR8jcvf92fVBSc+r+6aiPnB8Bt3s+mDF/Xytm6bxHmmacXXBa1fl9OdVhH2rpClm9j0zO1PSjyVtqKCPbzGzc7IvTmRm50iao+6binqDpEXZ7UWSXqywl2/olmm8G00zropfu8qnP3f3jl8k3aihb+T/R9I/VdFDg77+VtJ/Zpf3qu5N0nMaelv3fxp6R7RY0vmSNkn6ILs+r4t6+3dJ70p6R0PBGl9Rb3+noY+G70jakV1urPq1S/TVkdeNw2WBIDiCDgiCsANBEHYgCMIOBEHYgSAIOxAEYQeC+H/zBF3J5CSvrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[11].reshape(row,col), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(y_test[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Network\n",
    "\n",
    "Most peope can recognize handwrite digits with no problem(beacuse we have a very complex brain). This is not a easy task for a computer. We will use the neural netwok aproach, we will take a large number of labeled handwrite digits -the training examples- (<b>x_train</b>), and create a model that can learn from those examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons\n",
    "\n",
    " The idea is simple, they take several imputs(  $ x_1, x_2 ... x_n $) and produce a single output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single neuron](img/SingleNeuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input have  <i>weights</i>  $ w_1, w_2 ... w_n $ , a weight sum is calculated:${\\sum_n x_n w_n}$ then the results goes through a activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[http://neuralnetworksanddeeplearning.com/chap1.html](http://neuralnetworksanddeeplearning.com/chap1.html/) \n",
    "\n",
    "[https://github.com/ianmcloughlin/jupyter-teaching-notebooks/blob/master/keras-neurons.ipynb](https://github.com/ianmcloughlin/jupyter-teaching-notebooks/blob/master/keras-neurons.ipynb) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "We create layers composed of neurons. at least one input layer and a output layer is required, there can be hidden layers between the input and output layer. \n",
    "\n",
    "Our images are 28x28, we can create a 784 vector that represent the image. So we will feed out network with 784 inputs, this is our input layer that is also know as hidden layer.\n",
    "Since we need to discriminate 10 numbers out output layer will have 10 neurons, each will represent a digit from 0 to 9. Then we can use SoftMax activation function for the output of each of our network to represent a probability of being a digit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation : softmax\n",
    "  We want our result to represent a probablility of being one of the 10 digits(0,1,3...).\n",
    "  The softmax activation function will do this for us, it will output our output for each neuron beetwen 0 and 1.Then each \n",
    "  of our neuron can represent a a digit so the one will bigger probability will be our prediction.\n",
    "  \n",
    "  $$\\sigma (z)_j = \\frac{e^{(z)_j}}{\\sum_{k=0}^{K}e^{(z)_k}} \\text{  where   j = 1,...,K}$$  \n",
    "  \n",
    "  In our case K = 10.\n",
    "  \n",
    "  https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model\n",
    "\n",
    " We will start create a 2 layers model, input and output : the input layer will be the 784 vector created with the image data and the output a 10 neuros with activation softmax. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single neuron](img/Model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequeantial model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# The input layer is added by keras when we set input = 784 \n",
    "# We add 10 neuroas with softmax for output\n",
    "model.add(kr.layers.Dense(units=10, input_dim=784, activation='softmax'))\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are missing few thinks to explain: loss,optimizer and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(result, axis=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss : categorical_crossentropy\n",
    "\n",
    "    After we calculate our result we need to calculate the distance beetween our prediction and our epected result.\n",
    "    https://algorithmia.com/blog/introduction-to-loss-functions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimazer : adam\n",
    "https://algorithmia.com/blog/introduction-to-optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the labes\n",
    "    \n",
    "We are almost done for train our model for first time, the only problems is that our labels are a single digit. We need make theem vectors with a 1 on the number position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = kr.utils.to_categorical(y_train, 10)\n",
    "y_test  = kr.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready, we will start training this simple model with 10 epochs, batch of 100 (means we train the model with 100 images at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.7984 - accuracy: 0.7896 - val_loss: 0.4558 - val_accuracy: 0.8742\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.4288 - accuracy: 0.8787 - val_loss: 0.3679 - val_accuracy: 0.8991\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.3751 - accuracy: 0.8927 - val_loss: 0.3373 - val_accuracy: 0.9015\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.3525 - accuracy: 0.8982 - val_loss: 0.3276 - val_accuracy: 0.9051\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.3396 - accuracy: 0.9018 - val_loss: 0.3124 - val_accuracy: 0.9082\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.3304 - accuracy: 0.9043 - val_loss: 0.3304 - val_accuracy: 0.8991\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3236 - accuracy: 0.9071 - val_loss: 0.3119 - val_accuracy: 0.9098\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3180 - accuracy: 0.9085 - val_loss: 0.2993 - val_accuracy: 0.9159\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3117 - accuracy: 0.9102 - val_loss: 0.3155 - val_accuracy: 0.9090\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.3090 - accuracy: 0.9117 - val_loss: 0.2940 - val_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = np.array(history_callback.history['val_accuracy'])\n",
    "val_loss =  np.array(history_callback.history['val_loss'])\n",
    "accuracy =  np.array(history_callback.history['accuracy'])\n",
    "loss =  np.array(history_callback.history['loss'])\n",
    "def get_stats():\n",
    "    val_accuracy = np.array(history_callback.history['val_accuracy'])\n",
    "    val_loss =  np.array(history_callback.history['val_loss'])\n",
    "    accuracy =  np.array(history_callback.history['accuracy'])\n",
    "    loss =  np.array(history_callback.history['loss'])\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "getStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2456e196ec8>,\n",
       " <matplotlib.lines.Line2D at 0x2456e1a2188>,\n",
       " <matplotlib.lines.Line2D at 0x2456e1a2348>,\n",
       " <matplotlib.lines.Line2D at 0x2456e1a2508>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, epoch, 1)\n",
    "plt.plot(x,val_accuracy,x,accuracy,x,loss,x,val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(kr.callbacks.Callback):\n",
    "    minimun = 0.99\n",
    "   \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        score = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        # logs is a dictionary\n",
    "        print(f\"epoch: {epoch},history_callback.{logs['val_accuracy']}\")\n",
    "        print(score[1])\n",
    "        if score[1] > self.minimun: # your custom condition\n",
    "    \n",
    "            self.model.save('model7.h5', overwrite=True)\n",
    "           \n",
    "            print(\"saved\")\n",
    "            \n",
    "           \n",
    "            \n",
    "            self.minimun = score[1]\n",
    "            print(self.minimun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.4282 - accuracy: 0.8800 - val_loss: 0.3694 - val_accuracy: 0.8938\n",
      "epoch: 0,history_callback.0.8938000202178955\n",
      "0.8938000202178955\n"
     ]
    }
   ],
   "source": [
    "cbk = CustomModelCheckpoint()\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(units=512, input_dim=784, activation='relu'))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.5665 - accuracy: 0.8362 - val_loss: 0.3347 - val_accuracy: 0.9047\n",
      "epoch: 0,history_callback.0.904699981212616\n",
      "0.904699981212616\n"
     ]
    }
   ],
   "source": [
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100,\n",
    "                             callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = np.array(history_callback.history['val_accuracy'])\n",
    "val_loss =  np.array(history_callback.history['val_loss'])\n",
    "accuracy =  np.array(history_callback.history['accuracy'])\n",
    "loss =  np.array(history_callback.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1caa6c16e48>,\n",
       " <matplotlib.lines.Line2D at 0x1caa6c1dd48>,\n",
       " <matplotlib.lines.Line2D at 0x1caa6c1df08>,\n",
       " <matplotlib.lines.Line2D at 0x1caa6c27108>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, epoch, 1)\n",
    "plt.plot(x,val_accuracy,x,accuracy,x,loss,x,val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(units=512, input_dim=784, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.01))\n",
    "model.add(kr.layers.Dense(units=98, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.6029 - accuracy: 0.8127 - val_loss: 0.3369 - val_accuracy: 0.8985\n",
      "epoch: 0,history_callback.0.8985000252723694\n",
      "0.8985000252723694\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100\n",
    "                             , callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = np.array(history_callback.history['val_accuracy'])\n",
    "val_loss =  np.array(history_callback.history['val_loss'])\n",
    "accuracy =  np.array(history_callback.history['accuracy'])\n",
    "loss =  np.array(history_callback.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cce20ff648>,\n",
       " <matplotlib.lines.Line2D at 0x1cce2102608>,\n",
       " <matplotlib.lines.Line2D at 0x1cce21027c8>,\n",
       " <matplotlib.lines.Line2D at 0x1cce2102988>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOcUlEQVR4nO3df4zceV3H8eeLNoUgyN3RPcS20BKLsRDihUnV8AeHeNIjsUWOmDYxggr3hxYSBGMJhEDRKKcGY6wxlRDRhCsVola9pIHzLjEGsFvvQHq13FLALiWy/JAECdTq2z92Doe52d3vdmd2up8+H8lk5/v9fnb2/WmT501mdq6pKiRJG98Tpj2AJGk8DLokNcKgS1IjDLokNcKgS1IjNk/rB2/durV27tw5rR8vSRvS2bNnv1JVM6OuTS3oO3fuZHZ2dlo/XpI2pCRfWOqaL7lIUiMMuiQ1wqBLUiMMuiQ1wqBLUiM6BT3JviQXkswlOTLi+rOT3J/kU0keTLJ9/KNKkpazYtCTbAKOAXcCe4BDSfYMLfs94M+r6gXAUeC3xz2oJGl5XZ6h7wXmqupiVV0BTgAHhtbsAe7v339gxHVJ0oR1Cfo24NLA8Xz/3KBPAnf17/8s8NQkTx9+oCR3J5lNMruwsHAt80qSltAl6BlxbvhfxXgz8OIkDwEvBr4IXH3cN1Udr6peVfVmZkZ+clWSdI26fPR/HtgxcLwduDy4oKouA68ESPIU4K6q+sa4hpQkrazLM/QzwO4ku5JsAQ4CpwYXJNma5LHHegvwvvGOKUlayYpBr6qrwGHgNHAeOFlV55IcTbK/v+x24EKSzwDPAH5rQvNKkpaQaf0j0b1er/y/LUrS6iQ5W1W9Udf8pKgkNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjOgU9yb4kF5LMJTky4vqzkjyQ5KEkn0ry8vGPKklazopBT7IJOAbcCewBDiXZM7TsbcDJqroNOAj88bgHlSQtr8sz9L3AXFVdrKorwAngwNCaAr6/f/9pwOXxjShJ6qJL0LcBlwaO5/vnBr0D+Pkk88B9wOtHPVCSu5PMJpldWFi4hnElSUvpEvSMOFdDx4eAP6uq7cDLgb9I8rjHrqrjVdWrqt7MzMzqp5UkLalL0OeBHQPH23n8Syq/DJwEqKqPAU8Cto5jQElSN12CfgbYnWRXki0svul5amjNvwMvBUjyIywG3ddUJGkdrRj0qroKHAZOA+dZ/G2Wc0mOJtnfX/Ym4HVJPgncC7ymqoZflpEkTdDmLouq6j4W3+wcPPf2gfuPAC8a72iSpNXwk6KS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmN6BT0JPuSXEgyl+TIiOvvSfJw//aZJP85/lElScvZvNKCJJuAY8AdwDxwJsmpqnrksTVV9caB9a8HbpvArJKkZXR5hr4XmKuqi1V1BTgBHFhm/SHg3nEMJ0nqrkvQtwGXBo7n++ceJ8mzgV3APyxx/e4ks0lmFxYWVjurJGkZXYKeEedqibUHgQ9V1f+MulhVx6uqV1W9mZmZrjNKkjroEvR5YMfA8Xbg8hJrD+LLLZI0FV2CfgbYnWRXki0sRvvU8KIkPwzcDHxsvCNKkrpYMehVdRU4DJwGzgMnq+pckqNJ9g8sPQScqKqlXo6RJE3Qir+2CFBV9wH3DZ17+9DxO8Y3liRptfykqCQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiM6BT3JviQXkswlObLEmp9L8kiSc0k+MN4xJUkr2bzSgiSbgGPAHcA8cCbJqap6ZGDNbuAtwIuq6utJbp3UwJKk0bo8Q98LzFXVxaq6ApwADgyteR1wrKq+DlBVXx7vmJKklXQJ+jbg0sDxfP/coOcCz03yT0k+nmTfqAdKcneS2SSzCwsL1zaxJGmkLkHPiHM1dLwZ2A3cDhwC3pvkpsd9U9XxqupVVW9mZma1s0qSltEl6PPAjoHj7cDlEWv+pqr+u6o+B1xgMfCSpHXSJehngN1JdiXZAhwETg2t+WvgJQBJtrL4EszFcQ4qSVreikGvqqvAYeA0cB44WVXnkhxNsr+/7DTw1SSPAA8Av15VX53U0JKkx0vV8Mvh66PX69Xs7OxUfrYkbVRJzlZVb9Q1PykqSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUiE5BT7IvyYUkc0mOjLj+miQLSR7u3147/lElScvZvNKCJJuAY8AdwDxwJsmpqnpkaOkHq+rwBGaUJHXQ5Rn6XmCuqi5W1RXgBHBgsmNJklarS9C3AZcGjuf754bdleRTST6UZMdYppMkddYl6BlxroaO/xbYWVUvAD4KvH/kAyV3J5lNMruwsLC6SSVJy+oS9Hlg8Bn3duDy4IKq+mpVfad/+KfAC0c9UFUdr6peVfVmZmauZV5J0hK6BP0MsDvJriRbgIPAqcEFSZ45cLgfOD++ESVJXaz4Wy5VdTXJYeA0sAl4X1WdS3IUmK2qU8AbkuwHrgJfA14zwZklSSOkavjl8PXR6/VqdnZ2Kj9bkjaqJGerqjfqmp8UlaRGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGdAp6kn1JLiSZS3JkmXWvSlJJeuMbUZLUxYpBT7IJOAbcCewBDiXZM2LdU4E3AJ8Y95CSpJV1eYa+F5irqotVdQU4ARwYse5dwD3At8c4nySpoy5B3wZcGjie75/7riS3ATuq6u+We6AkdyeZTTK7sLCw6mElSUvrEvSMOFffvZg8AXgP8KaVHqiqjldVr6p6MzMz3aeUJK2oS9DngR0Dx9uBywPHTwWeDzyY5PPAjwOnfGNUktZXl6CfAXYn2ZVkC3AQOPXYxar6RlVtraqdVbUT+Diwv6pmJzKxJGmkFYNeVVeBw8Bp4DxwsqrOJTmaZP+kB5QkdbO5y6Kqug+4b+jc25dYe/vax5IkrZafFJWkRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWpEqmo6PzhZAL4wlR++NluBr0x7iHV2o+35RtsvuOeN5NlVNTPqwtSCvlElma2q3rTnWE832p5vtP2Ce26FL7lIUiMMuiQ1wqCv3vFpDzAFN9qeb7T9gntugq+hS1IjfIYuSY0w6JLUCIM+QpJbknwkyaP9rzcvse7V/TWPJnn1iOunknx68hOvzVr2m+TJSf4+yb8lOZfkd9Z3+tVJsi/JhSRzSY6MuP7EJB/sX/9Ekp0D197SP38hycvWc+61uNY9J7kjydkk/9r/+pPrPfu1Wsvfc//6s5J8M8mb12vmsagqb0M34B7gSP/+EeDdI9bcAlzsf725f//mgeuvBD4AfHra+5nkfoEnAy/pr9kC/CNw57T3tMQ+NwGfBZ7Tn/WTwJ6hNb8C/En//kHgg/37e/rrnwjs6j/OpmnvacJ7vg34wf795wNfnPZ+Jr3ngesfBv4SePO097Oam8/QRzsAvL9///3AK0aseRnwkar6WlV9HfgIsA8gyVOAXwN+cx1mHYdr3m9VfauqHgCoqivAvwDb12Hma7EXmKuqi/1ZT7C490GDfxYfAl6aJP3zJ6rqO1X1OWCu/3jXu2vec1U9VFWX++fPAU9K8sR1mXpt1vL3TJJXsPiE5dw6zTs2Bn20Z1TVlwD6X28dsWYbcGngeL5/DuBdwO8D35rkkGO01v0CkOQm4GeA+yc051qtuIfBNVV1FfgG8PSO33s9WsueB90FPFRV35nQnON0zXtO8n3AbwDvXIc5x27ztAeYliQfBX5gxKW3dn2IEecqyY8CP1RVbxx+XW6aJrXfgcffDNwL/GFVXVz9hOti2T2ssKbL916P1rLnxYvJ84B3Az89xrkmaS17fifwnqr6Zv8J+4Zywwa9qn5qqWtJ/iPJM6vqS0meCXx5xLJ54PaB4+3Ag8BPAC9M8nkW/3xvTfJgVd3OFE1wv485DjxaVX8whnEnZR7YMXC8Hbi8xJr5/n+kngZ8reP3Xo/WsmeSbAf+CviFqvrs5Mcdi7Xs+ceAVyW5B7gJ+N8k366qP5r82GMw7Rfxr8cb8Lt875uE94xYcwvwORbfGLy5f/+WoTU72Rhviq5pvyy+V/Bh4AnT3ssK+9zM4muju/j/N8ueN7TmV/neN8tO9u8/j+99U/QiG+NN0bXs+ab++rumvY/12vPQmnewwd4UnfoA1+ONxdcP7wce7X99LFw94L0D636JxTfH5oBfHPE4GyXo17xfFp/9FHAeeLh/e+2097TMXl8OfIbF34J4a//cUWB///6TWPzthjngn4HnDHzvW/vfd4Hr9Dd5xrln4G3Afw38vT4M3Drt/Uz673ngMTZc0P3ovyQ1wt9ykaRGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RG/B/CumQQe7gfIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x = np.arange(0.0, epoch, 1)\n",
    "plt.plot(x,val_accuracy,x,accuracy,x,loss,x,val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_13_input to have 4 dimensions, but got array with shape (10000, 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-539b1b0b5424>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model1.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_13_input to have 4 dimensions, but got array with shape (10000, 784)"
     ]
    }
   ],
   "source": [
    "score = kr.models.load_model('./model1.h5').evaluate(x_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(train_img_total,col,row,1)\n",
    "x_test = x_test.reshape(test_img_total,col,row,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Conv2D(128,kernel_size=(5, 5),activation='relu'))\n",
    "model.add(kr.layers.Conv2D(256,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dropout(0.25))\n",
    "model.add(kr.layers.Dense(units=98, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.2193 - accuracy: 0.9301 - val_loss: 0.0546 - val_accuracy: 0.9828\n",
      "epoch: 0,history_callback.0.9828000068664551\n",
      "0.9828000068664551\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100\n",
    "                             , callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04757987779821233, 0.9961000084877014]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = kr.models.load_model('./model1.h5').evaluate(x_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Conv2D(128,kernel_size=(5, 5),activation='relu'))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(256,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dropout(0.25))\n",
    "model.add(kr.layers.Dense(units=98, activation='relu'))\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.2160 - accuracy: 0.9311 - val_loss: 0.0476 - val_accuracy: 0.9853\n",
      "epoch: 0,history_callback.0.9853000044822693\n",
      "0.9853000044822693\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "history_callback = model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=epoch, batch_size=100\n",
    "                             , callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04757987779821233, 0.9961000084877014]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = kr.models.load_model('./model1.h5').evaluate(x_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 22, 22, 64)        3200      \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 16, 16, 64)        200768    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 2, 2, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 706,634\n",
      "Trainable params: 706,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "kr.layers.normalization.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(7, 7),activation='relu',input_shape=(28,28,1)))\n",
    "kr.layers.normalization.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Conv2D(128,kernel_size=(5, 5),activation='relu'))\n",
    "model.add(kr.layers.Conv2D(256,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "model.add(kr.layers.Dropout(0.25))\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(loss=kr.losses.categorical_crossentropy,optimizer='adadelta',metrics=['accuracy'])\n",
    "model.summary()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.flow(x_train, y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 3.7253e-09 - val_accuracy: 0.9944\n",
      "epoch: 0,history_callback.0.9943910241127014\n",
      "0.9944000244140625\n",
      "Epoch 2/10\n",
      "937/937 [==============================] - 12s 12ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 1.3038e-07 - val_accuracy: 0.9950\n",
      "epoch: 1,history_callback.0.9949678182601929\n",
      "0.9950000047683716\n",
      "Epoch 3/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 6.8649e-06 - val_accuracy: 0.9945\n",
      "epoch: 2,history_callback.0.9944645762443542\n",
      "0.9944999814033508\n",
      "Epoch 4/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0111 - accuracy: 0.9975 - val_loss: 3.4880e-05 - val_accuracy: 0.9948\n",
      "epoch: 3,history_callback.0.9947665333747864\n",
      "0.9947999715805054\n",
      "Epoch 5/10\n",
      "937/937 [==============================] - 11s 12ms/step - loss: 0.0094 - accuracy: 0.9978 - val_loss: 5.9605e-08 - val_accuracy: 0.9951\n",
      "epoch: 4,history_callback.0.9950684309005737\n",
      "0.995199978351593\n",
      "Epoch 6/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0092 - accuracy: 0.9979 - val_loss: 0.0000e+00 - val_accuracy: 0.9949\n",
      "epoch: 5,history_callback.0.9948671460151672\n",
      "0.9948999881744385\n",
      "Epoch 7/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0108 - accuracy: 0.9978 - val_loss: 7.4506e-09 - val_accuracy: 0.9956\n",
      "epoch: 6,history_callback.0.9955716729164124\n",
      "0.9955999851226807\n",
      "Epoch 8/10\n",
      "937/937 [==============================] - 11s 12ms/step - loss: 0.0091 - accuracy: 0.9978 - val_loss: 1.4901e-08 - val_accuracy: 0.9948\n",
      "epoch: 7,history_callback.0.9947665333747864\n",
      "0.9950000047683716\n",
      "Epoch 9/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 6.0282e-06 - val_accuracy: 0.9948\n",
      "epoch: 8,history_callback.0.9947665333747864\n",
      "0.9945999979972839\n",
      "Epoch 10/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 1.2090e-04 - val_accuracy: 0.9950\n",
      "epoch: 9,history_callback.0.9949678182601929\n",
      "0.9948999881744385\n"
     ]
    }
   ],
   "source": [
    "epoch =10\n",
    "history_callback= model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=epoch, \n",
    "                    validation_data=test_generator, validation_steps=10000//64, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_25_input to have 4 dimensions, but got array with shape (10000, 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-cae98f6b7e17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model7.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_25_input to have 4 dimensions, but got array with shape (10000, 784)"
     ]
    }
   ],
   "source": [
    "score = kr.models.load_model('./model7.h5').evaluate(x_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
